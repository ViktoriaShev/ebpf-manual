## Hook Points (Точки прикрепления) eBPF программ**

В eBPF программы являются событийно-ориентированными и выполняются при прохождении ядром или приложением определенных точек прикрепления (hook points). Эти точки позволяют внедрять пользовательскую логику в различные этапы работы системы.

### **1. Основные категории Hook Points**

#### **1.1. Трассировка (Tracing)**
Программы для наблюдения за системой без влияния на её работу:

- **Kprobes/Kretprobes**  
  Прикрепление к:
  - Входу в функцию ядра (`kprobe`)
  - Выходу из функции ядра (`kretprobe`)
  ```c
  SEC("kprobe/do_sys_openat2")
  int kprobe__do_sys_openat2(struct pt_regs *ctx) { ... }
  ```

- **Tracepoints**  
  Стабильные точки трассировки в ядре:
  ```c
  SEC("tracepoint/syscalls/sys_enter_openat")
  int tp_sys_enter_openat(struct trace_event_raw_sys_enter *ctx) { ... }
  ```

- **Fentry/Fexit**  
  Более эффективная альтернатива kprobes (требует ядро 5.5+):
  ```c
  SEC("fentry/do_unlinkat")
  int BPF_PROG(fentry_do_unlinkat, int dfd, struct filename *name) { ... }
  ```

- **USDT (User Statically Defined Tracepoints)**  
  Точки трассировки в пользовательских приложениях:
  ```c
  SEC("uprobe//usr/bin/python3.8:main")
  int uprobe__python_main(struct pt_regs *ctx) { ... }
  ```

#### **1.2. Сетевые (Networking)**
Программы для обработки сетевого трафика:

- **XDP (eXpress Data Path)**  
  Обработка пакетов на самом раннем этапе (до выделения SKB):
  ```c
  SEC("xdp")
  int xdp_drop(struct xdp_md *ctx) { ... }
  ```

- **TC (Traffic Control)**  
  Фильтрация и классификация пакетов на уровне сетевого стека:
  ```c
  SEC("tc")
  int tc_ingress(struct __sk_buff *skb) { ... }
  ```

- **Socket Filter**  
  Фильтрация трафика на уровне сокетов:
  ```c
  SEC("socket")
  int socket_filter(struct __sk_buff *skb) { ... }
  ```

- **Cgroup**  
  Управление сетевыми политиками для контрольных групп:
  ```c
  SEC("cgroup/skb")
  int cgroup_skb_egress(struct __sk_buff *skb) { ... }
  ```

### **2. Специфические Hook Points**

### **2.1. LSM (Linux Security Module)**
Прикрепление к точкам проверки безопасности ядра:
```c
SEC("lsm/socket_bind")
int lsm_socket_bind(struct bpf_sock *sk) { ... }
```

### **2.2. Flow Dissector**
Анализ заголовков сетевых пакетов:
```c
SEC("flow_dissector")
int flow_dissector(struct __sk_buff *skb) { ... }
```

### **2.3. LWT (Lightweight Tunnels)**
Обработка инкапсулированного трафика:
```c
SEC("lwt_in")
int lwt_in_func(struct __sk_buff *skb) { ... }
```

### **3. Как выбирать Hook Point?**

| Критерий | Рекомендуемый Hook |
|----------|---------------------|
| **Ранняя обработка пакетов** | XDP |
| **Фильтрация на сетевом уровне** | TC |
| **Аудит системных вызовов** | Tracepoints |
| **Профилирование функций** | Kprobes/Fentry |
| **Безопасность** | LSM |
| **Пользовательские приложения** | Uprobes/USDT |

### **4. Особенности работы**

1. **Доступ к контексту**  
   Каждый тип hook point предоставляет свой контекст:
   - `struct xdp_md` для XDP
   - `struct __sk_buff` для TC
   - `struct pt_regs` для kprobes

2. **Ограничения**  
   - XDP работает только с сырыми пакетами (без метаданных SKB)
   - Kprobes могут быть нестабильными между версиями ядра

3. **Производительность**  
   - XDP и Fentry имеют наименьшие накладные расходы
   - Kprobes могут замедлять систему при частых вызовах

### **5. Примеры использования**

#### **Блокировка SSH-подключений через XDP**
```c
SEC("xdp") 
int xdp_block_ssh(struct xdp_md *ctx) {
    if (is_ssh_packet(ctx)) 
        return XDP_DROP;
    return XDP_PASS;
}
```

#### **Аудит открытия файлов через Tracepoint**
```c
SEC("tracepoint/syscalls/sys_enter_openat")
int tp_openat(struct trace_event_raw_sys_enter *ctx) {
    char *filename = (char *)ctx->args[1];
    bpf_printk("File opened: %s", filename);
    return 0;
}
```

Подробнее о програмных типах можно узнать в [документации ядра](https://www.kernel.org/doc/html/latest/bpf/prog_types.html).

## Трассировки
Точки трассировки — это метод статического инструментирования ядра, технически просто функции трассировки, размещенные в исходном коде ядра, которые по сути являются точками зондирования с контрольными условиями, вставленными в исходный код, что позволяет выполнять постобработку с дополнительными функциями обработки. Например, наиболее распространенным методом статической трассировки в ядре является printk, который выводит сообщения журнала. Например, существуют точки трассировки в начале и конце системных вызовов, событий планировщика, операций файловой системы и дискового ввода-вывода. Точки трассировки были впервые представлены в Linux версии 2.6.32 в 2009 году. Точки трассировки — это стабильный API, и их количество ограничено.

Можно увидеть доступный набор подсистем трассировки в ядре, просмотрев /sys/kernel/tracing/available_events

## Закрепление
Загрузка программы BPF в ядро ​​с помощью системного вызова bpf() возвращает файловый дескриптор. В ядре этот файловый дескриптор является ссылкой на программу.

Процесс пользовательского пространства, который сделал системный вызов, владеет этим файловым дескриптором; когда этот процесс завершается, файловый дескриптор освобождается, а счетчик ссылок на программу уменьшается. Когда не остается ссылок на программу BPF, ядро удаляет программу.

Дополнительная ссылка создается, когда вы закрепляете программу в файловой системе.

Закрепление ее в файловой системе означает, что есть дополнительная ссылка на программу, поэтому программа остается загруженной после завершения команды.

Счетчик ссылок также увеличивается, когда программа BPF присоединена к хуку, который ее вызовет. Поведение этих счетчиков ссылок зависит от типа программы BPF. Eсть некоторые типы, которые относятся к трассировке (например, kprobes и tracepoints) и всегда связаны с процессом пользовательского пространства; для этих типов программ eBPF счетчик ссылок ядра уменьшается при завершении этого процесса. Программы, которые прикреплены внутри сетевого стека или cgroups (сокращение от «контрольные группы»), не связаны ни с каким процессом пользовательского пространства, поэтому они остаются на месте даже после завершения работы программы пользовательского пространства, которая их загружает. 

Загрузка программы XDP с помощью команды ip link:

```

ip link set dev enp0s3 xdp obj xdp_filter.o sec xdp

```

Команда ip завершена, и нет определения закрепленного местоположения, но тем не менее, bpftool покажет вам, что программа XDP загружена в ядро:

```

$ bpftool prog list

```

Счетчик ссылок для этой программы не равен нулю из-за присоединения к хуку XDP, которое сохранилось после завершения команды ip link.

Карты eBPF также имеют счетчики ссылок, и они очищаются, когда их счетчик ссылок падает до нуля. Каждая программа eBPF, которая использует карту, увеличивает счетчик, как и каждый файловый дескриптор, который программы пользовательского пространства могут хранить для карты.

Возможно, исходный код программы eBPF может определять карту, на которую программа на самом деле не ссылается. Если программа eBPF ничего не делает с картой, автоматически не будет счетчика ссылок из программы на карту. Существует системный вызов BPF(BPF_PROG_BIND_MAP), который связывает карту с программой, так что карта не очищается, как только программа-загрузчик пользовательского пространства завершает работу и больше не хранит ссылку на дескриптор файла для карты.

Карты также могут быть закреплены в файловой системе, и программы пользовательского пространства могут получить доступ к

карте, зная путь к карте.

## CO-RE подход
### CORE approach: compile once, run everywhere

Многие программы eBPF обращаются к структурам данных ядра, и программисту eBPF необходимо включить соответствующие файлы заголовков Linux, чтобы их код eBPF мог правильно находить поля в этих структурах данных. Однако ядро ​​Linux находится в стадии непрерывной разработки, что означает, что внутренние структуры данных могут меняться между разными версиями ядра. Если взять объектный файл eBPF, скомпилированный на одной машине и загрузить его на машину с другой версией ядра, не было бы гарантии, что структуры данных будут такими же.

Подход CO-RE — позволяет программам eBPF включать информацию о макетах структуры данных, с которыми они были скомпилированы, и предоставляет механизм для настройки того, как доступ к полям, если макет структуры данных отличается на целевой машине, где они работают. При условии, что программа не хочет получать доступ к полю или структуре данных, которые просто не существуют в ядре целевой машины, программа переносима между разными версиями ядра.

Подход CO-RE состоит из нескольких элементов:
### BTF

BTF — это формат для выражения макета структур данных и сигнатур функций. В CO-RE он используется для определения любых различий между структурами, используемыми во время компиляции и во время выполнения. BTF также используется такими инструментами, как bpftool, для дампа структур данных в форматах, удобных для восприятия человеком. Ядра Linux, начиная с версии 5.4, поддерживают BTF.

Когда ядро Linux компилируется, оно генерирует BTF-метаданные, которые представляют типы данных, используемые в ядре. Эти данные включают в себя описания структур, типов, функций и их полей. BTF-метаданные сохраняются в специальном формате, который ядро может использовать, а также могут быть доступны программам BPF для анализа и использования.
### Заголовки ядра

Исходный код ядра Linux включает файлы заголовков, которые описывают используемые им структуры данных, и эти заголовки могут меняться между версиями Linux. Программисты eBPF могут включить отдельные файлы заголовков или можно использовать bpftool для генерации файла заголовка vmlinux.h из

работающей системы, содержащего всю информацию о структуре данных ядра, которая может понадобиться программе BPF.
#### vmlinux.h

Программа eBPF нуждается в определениях всех структур данных ядра и типов, на которые она будет ссылаться. Есть относително простой способ выяснить, какой из множества отдельных файлов заголовков в исходном коде ядра Linux содержал определение структур, которые необходимы для программ, и это - генерация файл заголовка vmlinux.h

Его можно сгенерировать его с помощью bpftool следующим образом:

```bash

bpftool btf dump file /sys/kernel/btf/vmlinux format c > vmlinux.h

```

Этот файл определяет все типы данных ядра, поэтому включение этого сгенерированного файла vmlinux.h в исходный код программы eBPF обеспечивает определения любых структур данных Linux, которые могут понадобиться. Когда компилируется исходный код в объектный файл eBPF, этот объект будет включать информацию BTF, которая соответствует определениям, используемым в этом заголовочном файле.

Позже, когда программа запускается на целевой машине, программа пользовательского пространства, которая загружает ее в ядро, внесет корректировки, чтобы учесть различия между этой информацией BTF времени сборки и информацией BTF для ядра, которое работает на этой целевой машине.

Информация BTF в виде файла /sys/kernel/btf/vmlinux была включена в ядро Linux с версии 5.4,6, но необработанные данные BTF, которые может использовать libbpf, также могут быть сгенерированы для более старых ядер.

### Поддержка компилятора

Компилятор Clang был улучшен таким образом, что при компиляции программ eBPF с флагом -g он включает то, что известно как перемещения CO-RE, полученные из информации BTF, описывающей структуры данных ядра. Компилятор GCC также добавил поддержку CO-RE для целей BPF в версии 12.
### Поддержка библиотек для перемещений структур данных

В момент, когда программа пользовательского пространства загружает программу eBPF в ядро, подход CO-RE требует корректировки байт-кода для компенсации любых различий между структурами данных, присутствующими при компиляции, и тем, что находится на целевой машине, где он собирается запуститься, на основе информации о перемещении CO-RE, скомпилированной в объект. Есть несколько библиотек, которые позаботятся об этом: libbpf была оригинальной библиотекой C, которая включает эту возможность перемещения, библиотека Cilium eBPF предоставляет ту же возможность для программистов Go, а Aya делает это для Rust.
### Опционально, скелет BPF

Скелет может быть автоматически сгенерирован из скомпилированного объектного файла BPF, содержащего удобные функции, которые код пользовательского пространства может вызывать для управления жизненным циклом программ BPF — загрузки их в ядро, присоединения их к событиям и т. д. Если вы пишете код пользовательского пространства на C, вы можете сгенерировать скелет с помощью bpftool gen skeleton. Эти функции являются абстракциями более высокого уровня, которые могут быть более удобными для разработчика, чем использование базовой библиотеки (libbpf, cilium/ebpf и т. д.) напрямую.
## XDP (eXpress Data Path)

![[Pasted image 20250415160224.png]]  
[книга ](Learning eBPF Programming the Linux Kernel for Enhanced Observability, Networking, and Security (Liz Rice).pdf)
### **1. Основные концепции XDP**

### **1.1. Что такое XDP?**

XDP (eXpress Data Path) — это высокопроизводительный механизм обработки пакетов в ядре Linux, работающий на самом раннем этапе получения сетевого трафика. Давайте разберём его внутреннее устройство.

- **Работает на уровне драйвера сетевой карты** (до выделения SKB)
- **Обрабатывает пакеты "на лету"** с минимальными накладными расходами
- **Поддерживает аппаратное ускорение** (оффлоад на NIC)

### **2. Режимы работы XDP**

| Режим           | Описание                    | Производительность  | Требования          |
| --------------- | --------------------------- | ------------------- | ------------------- |
| **Native XDP**  | Обработка в драйвере NIC    | Максимальная        | Поддержка драйвером |
| **Generic XDP** | Эмуляция в ядре             | Средняя             | Любые интерфейсы    |
| **Offload XDP** | Выполнение на сетевой карте | Аппаратная скорость | Спец. оборудование  |

XDP работает **до создания SKB (socket buffer)** — структур ядра, используемых для традиционной обработки пакетов. Это происходит на одном из трёх уровней:

1. **Driver Level (Native XDP)**  
   - Обработка прямо в драйвере сетевой карты
   - Пакеты перехватываются **до выделения DMA-буферов**
   - Максимальная производительность

2. **Generic XDP**  
   - Эмуляция XDP в ядре, если драйвер не поддерживает Native XDP
   - Работает после создания `sk_buff`, но до сетевого стека

3. **Hardware Offload (SmartNIC)**  
   - Выполнение eBPF-программ **на самом сетевом адаптере**
   - Zero-CPU обработка (например, NVIDIA ConnectX-6 Dx)

```plaintext
       +---------------------------------------------------+
       |                   Сетевой адаптер                 |
       +---------------------------------------------------+
                      | (DMA)
       +---------------------------------------------------+
       |              Драйвер сетевой карты                |
       |  +---------------------------------------------+  |
       |  |               XDP (Native)                  |  |
       |  |   - Обработка до создания SKB               |  |
       |  |   - Прямой доступ к ring-буферам NIC        |  |
       |  +---------------------------------------------+  |
       +---------------------------------------------------+
                      | (netif_receive_skb)
       +---------------------------------------------------+
       |              Ядро Linux (Generic XDP)             |
       |  +---------------------------------------------+  |
       |  |      Обработка после skb_alloc(),           |  |
       |  |      но до netif_receive_skb()              |  |
       |  +---------------------------------------------+  |
       +---------------------------------------------------+
```

![[Pasted image 20250415154855.png]]

### **3. Жизненный цикл пакета в XDP**

### **3.1. Получение пакета**
1. Сетевая карта (NIC) получает пакет и помещает его в **DMA-буфер**.
2. Драйвер NIC:
   - Для **Native XDP**: вызывает eBPF-программу **сразу после DMA**, передавая указатель на сырые данные.
   - Для **Generic XDP**: сначала создаёт `sk_buff`, затем передаёт в XDP.

### **3.2 Возвращаемые коды (verdicts)**

Каждая XDP-программа должна вернуть одно из решений:

```c
#define XDP_PASS     1  // Пропустить в сетевой стек
#define XDP_DROP     2  // Отбросить пакет
#define XDP_TX       3  // Отправить обратно в интерфейс
#define XDP_REDIRECT 4  // Перенаправить в другой интерфейс
#define XDP_ABORTED  5  // Аварийное завершение
```

### **4. Структура XDP-программы**

### **4.1. Структура `struct xdp_md`**
Контекст, передаваемый в XDP-программу:
```c
struct xdp_md {
    __u32 data;         // Указатель на начало пакета
    __u32 data_end;     // Конец пакета
    __u32 data_meta;    // Метаданные (используется для CTF)
    __u32 ingress_ifindex; // Индекс интерфейса
    __u32 rx_queue_index;  // Очередь RX
    __u32 egress_ifindex;  // Для XDP_REDIRECT
};
```
- **data** и **data_end** позволяют безопасно читать пакет (верификатор проверяет границы).
### **4.2. Базовый шаблон**
```c
#include <linux/bpf.h>
#include <bpf/bpf_helpers.h>

SEC("xdp")
int xdp_prog(struct xdp_md *ctx) {
    // Анализ пакета
    void *data = (void *)(long)ctx->data;
    void *data_end = (void *)(long)ctx->data_end;
    
    // Проверка границ (обязательно!)
    if (data + sizeof(struct ethhdr) > data_end)
        return XDP_ABORTED;

    // Логика обработки...
    return XDP_PASS;
}

char __license[] SEC("license") = "GPL";
```

### **4.3. Доступ к пакету**
- **ctx->data**: Указатель на начало пакета
- **ctx->data_end**: Конец пакета
- **Проверка границ обязательна** для верификатора

### **5. Практические применения XDP**

### **5.1. Фильтрация трафика (Firewall)**
```c
SEC("xdp")
int xdp_firewall(struct xdp_md *ctx) {
    struct ethhdr *eth = ctx->data;
    struct iphdr *ip = ctx->data + sizeof(*eth);
    
    // Блокировка ICMP (ping)
    if (ip->protocol == IPPROTO_ICMP)
        return XDP_DROP;
        
    return XDP_PASS;
}
```

### **5.2. Балансировка нагрузки**
```c
SEC("xdp")
int xdp_lb(struct xdp_md *ctx) {
    // Выбор бэкенда (50/50)
    __u32 backend = bpf_get_prandom_u32() % 2 ? BACKEND_A : BACKEND_B;
    
    // Модификация IP/MAC
    modify_headers(ctx, backend);
    
    return XDP_TX;  // Отправка обратно
}
```

### **5.3. DDoS-защита**
```c
SEC("xdp")
int xdp_anti_ddos(struct xdp_md *ctx) {
    struct iphdr *ip = ctx->data + sizeof(struct ethhdr);
    __u32 sip = ip->saddr;
    
    // Проверка rate-limiting
    if (bpf_map_lookup_elem(&rate_map, &sip)) {
        bpf_map_update_elem(&rate_map, &sip, &counter, BPF_ANY);
        return XDP_DROP;
    }
    
    return XDP_PASS;
}
```

### **6. Производительность XDP**

#### **6.1 Ключевые оптимизации производительности**

##### **3.1. Zero-Copy обработка**
- В Native XDP пакеты **не копируются** между пользовательским и ядерным пространством.
- Данные читаются напрямую из DMA-буферов.

##### **3.2. Отсутствие блокировок**
- XDP работает в **контексте прерывания** (softirq).
- Нет мьютексов/спинлоков — только атомарные операции.

##### **3.3. Batch-обработка**
- Современные драйверы (например, `ice`, `mlx5`) обрабатывают пакеты пачками (до 64 за вызов).
#### **Сравнение с традиционными методами**

| Метод | Скорость (pps) | Задержка | CPU Нагрузка |
|-------|----------------|----------|--------------|
| iptables | 1-2M | Высокая | Высокая |
| Userspace (DPDK) | 10-20M | Низкая | Средняя |
| XDP (Native) | 20-40M | Очень низкая | Низкая |
| XDP (Offload) | 100M+ | Минимальная | ~0% |

### **7. Ограничения и подводные камни**

8. **Нет доступа к сетевому стеку**:
   - Нельзя использовать `sockets`, `iptables`, `conntrack`.
   
2. **Ограниченный набор helper-функций**:
   - Только безопасные операции (например, `bpf_redirect_map`).

3. **Сложность отладки**:
   - Нет доступа к `printk`, только `bpf_trace_printk` (ограниченный буфер).

4. **Зависимость от драйверов**:
   - Не все NIC поддерживают Native XDP (проверять `ethtool -i eth0`).


### **8. Инструменты для работы с XDP**

- **Загрузка программы**:
  ```bash
  bpftool prog load xdp_prog.o /sys/fs/bpf/xdp_prog
  bpftool net attach xdp pinned /sys/fs/bpf/xdp_prog dev eth0
  ```

- **Просмотр активных программ**:
  ```bash
  ip link show eth0
  ```

- **Удаление**:
  ```bash
  ip link set dev eth0 xdp off
  ```

